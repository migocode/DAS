{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as time\n",
    "import statistics\n",
    "from tabulate import tabulate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.00000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.00000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.833893</td>\n",
       "      <td>0.431438</td>\n",
       "      <td>581.839465</td>\n",
       "      <td>0.418060</td>\n",
       "      <td>38.083612</td>\n",
       "      <td>0.351171</td>\n",
       "      <td>263358.029264</td>\n",
       "      <td>1.39388</td>\n",
       "      <td>136.625418</td>\n",
       "      <td>0.648829</td>\n",
       "      <td>0.32107</td>\n",
       "      <td>130.260870</td>\n",
       "      <td>0.32107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.894809</td>\n",
       "      <td>0.496107</td>\n",
       "      <td>970.287881</td>\n",
       "      <td>0.494067</td>\n",
       "      <td>11.834841</td>\n",
       "      <td>0.478136</td>\n",
       "      <td>97804.236869</td>\n",
       "      <td>1.03451</td>\n",
       "      <td>4.412477</td>\n",
       "      <td>0.478136</td>\n",
       "      <td>0.46767</td>\n",
       "      <td>77.614208</td>\n",
       "      <td>0.46767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25100.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>116.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>212500.000000</td>\n",
       "      <td>0.90000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>262000.000000</td>\n",
       "      <td>1.10000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>303500.000000</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7861.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>850000.000000</td>\n",
       "      <td>9.40000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age     anaemia  creatinine_phosphokinase    diabetes  \\\n",
       "count  299.000000  299.000000                299.000000  299.000000   \n",
       "mean    60.833893    0.431438                581.839465    0.418060   \n",
       "std     11.894809    0.496107                970.287881    0.494067   \n",
       "min     40.000000    0.000000                 23.000000    0.000000   \n",
       "25%     51.000000    0.000000                116.500000    0.000000   \n",
       "50%     60.000000    0.000000                250.000000    0.000000   \n",
       "75%     70.000000    1.000000                582.000000    1.000000   \n",
       "max     95.000000    1.000000               7861.000000    1.000000   \n",
       "\n",
       "       ejection_fraction  high_blood_pressure      platelets  \\\n",
       "count         299.000000           299.000000     299.000000   \n",
       "mean           38.083612             0.351171  263358.029264   \n",
       "std            11.834841             0.478136   97804.236869   \n",
       "min            14.000000             0.000000   25100.000000   \n",
       "25%            30.000000             0.000000  212500.000000   \n",
       "50%            38.000000             0.000000  262000.000000   \n",
       "75%            45.000000             1.000000  303500.000000   \n",
       "max            80.000000             1.000000  850000.000000   \n",
       "\n",
       "       serum_creatinine  serum_sodium         sex    smoking        time  \\\n",
       "count         299.00000    299.000000  299.000000  299.00000  299.000000   \n",
       "mean            1.39388    136.625418    0.648829    0.32107  130.260870   \n",
       "std             1.03451      4.412477    0.478136    0.46767   77.614208   \n",
       "min             0.50000    113.000000    0.000000    0.00000    4.000000   \n",
       "25%             0.90000    134.000000    0.000000    0.00000   73.000000   \n",
       "50%             1.10000    137.000000    1.000000    0.00000  115.000000   \n",
       "75%             1.40000    140.000000    1.000000    1.00000  203.000000   \n",
       "max             9.40000    148.000000    1.000000    1.00000  285.000000   \n",
       "\n",
       "       DEATH_EVENT  \n",
       "count    299.00000  \n",
       "mean       0.32107  \n",
       "std        0.46767  \n",
       "min        0.00000  \n",
       "25%        0.00000  \n",
       "50%        0.00000  \n",
       "75%        1.00000  \n",
       "max        1.00000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'G:\\My Drive\\FH_Technikum\\MSC\\Semester_2_SS2022\\DAS\\ComparativeExperimentation\\heart_failure_clinical_records_dataset.csv')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testdata as well as columns that should be predicted (y) and columns that contain data that will be used to predict (X). (= holdout method)\n",
    "\n",
    "The columns that should be predicted (target/dependent) must be excluded from the trainingsdata to not influence the created modle.\n",
    "\n",
    "The dependent (to be predicted) data is located in column 54 (Forest Cover Type Classes => values from 1 to 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (200, 11)\n",
      "X_test: (99, 11)\n",
      "y_train: (200, 1)\n",
      "y_test: (99, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.loc[:,:'smoking'], data.loc[:,'DEATH_EVENT':], test_size=0.33, random_state=547998)\n",
    "print(\"X_train: \" + str(X_train.shape))\n",
    "print(\"X_test: \" + str(X_test.shape))\n",
    "print(\"y_train: \" + str(y_train.shape))\n",
    "print(\"y_test: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data/discussion/178372 discussion regarding the data (the time column), it should be excluded because it is directly connected to the prediction variable DEATH_EVENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "I chose to vary the parameters for min_samples_splits and min_samples_leafs to see the differences between the different values since they seem to be the most promising to have an impact on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run-parameters min_samples_split: [2] min_samples_leaf: [1]\n",
      "-------------------------------------------\n",
      "training time: 0.0059909820556640625 seconds\n",
      "testing time: 0.0010046958923339844 seconds\n",
      "accuracy: 0.696969696969697\n",
      "micro f-score: 0.696969696969697\n",
      "macro f-score: 0.641304347826087\n",
      "weighted f-score: 0.699824330259113\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [2] min_samples_leaf: [50]\n",
      "-------------------------------------------\n",
      "training time: 0.003996372222900391 seconds\n",
      "testing time: 0.001003265380859375 seconds\n",
      "accuracy: 0.696969696969697\n",
      "micro f-score: 0.696969696969697\n",
      "macro f-score: 0.641304347826087\n",
      "weighted f-score: 0.699824330259113\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [2] min_samples_leaf: [100]\n",
      "-------------------------------------------\n",
      "training time: 0.0020127296447753906 seconds\n",
      "testing time: 0.0029931068420410156 seconds\n",
      "accuracy: 0.696969696969697\n",
      "micro f-score: 0.696969696969697\n",
      "macro f-score: 0.641304347826087\n",
      "weighted f-score: 0.699824330259113\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [2] min_samples_leaf: [1000]\n",
      "-------------------------------------------\n",
      "training time: 0.0030035972595214844 seconds\n",
      "testing time: 0.0010013580322265625 seconds\n",
      "accuracy: 0.696969696969697\n",
      "micro f-score: 0.696969696969697\n",
      "macro f-score: 0.641304347826087\n",
      "weighted f-score: 0.699824330259113\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [50] min_samples_leaf: [1]\n",
      "-------------------------------------------\n",
      "training time: 0.001992464065551758 seconds\n",
      "testing time: 0.00401759147644043 seconds\n",
      "accuracy: 0.7676767676767676\n",
      "micro f-score: 0.7676767676767676\n",
      "macro f-score: 0.7405719494132392\n",
      "weighted f-score: 0.7752999978133851\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [50] min_samples_leaf: [50]\n",
      "-------------------------------------------\n",
      "training time: 0.0020132064819335938 seconds\n",
      "testing time: 0.0009949207305908203 seconds\n",
      "accuracy: 0.7676767676767676\n",
      "micro f-score: 0.7676767676767676\n",
      "macro f-score: 0.7405719494132392\n",
      "weighted f-score: 0.7752999978133851\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [50] min_samples_leaf: [100]\n",
      "-------------------------------------------\n",
      "training time: 0.0020003318786621094 seconds\n",
      "testing time: 0.0 seconds\n",
      "accuracy: 0.7676767676767676\n",
      "micro f-score: 0.7676767676767676\n",
      "macro f-score: 0.7405719494132392\n",
      "weighted f-score: 0.7752999978133851\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [50] min_samples_leaf: [1000]\n",
      "-------------------------------------------\n",
      "training time: 0.004988670349121094 seconds\n",
      "testing time: 0.0 seconds\n",
      "accuracy: 0.7676767676767676\n",
      "micro f-score: 0.7676767676767676\n",
      "macro f-score: 0.7405719494132392\n",
      "weighted f-score: 0.7752999978133851\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [100] min_samples_leaf: [1]\n",
      "-------------------------------------------\n",
      "training time: 0.0020008087158203125 seconds\n",
      "testing time: 0.0 seconds\n",
      "accuracy: 0.7676767676767676\n",
      "micro f-score: 0.7676767676767676\n",
      "macro f-score: 0.7405719494132392\n",
      "weighted f-score: 0.7752999978133851\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [100] min_samples_leaf: [50]\n",
      "-------------------------------------------\n",
      "training time: 0.0020024776458740234 seconds\n",
      "testing time: 0.0009980201721191406 seconds\n",
      "accuracy: 0.7676767676767676\n",
      "micro f-score: 0.7676767676767676\n",
      "macro f-score: 0.7405719494132392\n",
      "weighted f-score: 0.7752999978133851\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [100] min_samples_leaf: [100]\n",
      "-------------------------------------------\n",
      "training time: 0.005003452301025391 seconds\n",
      "testing time: 0.0010001659393310547 seconds\n",
      "accuracy: 0.7676767676767676\n",
      "micro f-score: 0.7676767676767676\n",
      "macro f-score: 0.7405719494132392\n",
      "weighted f-score: 0.7752999978133851\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [100] min_samples_leaf: [1000]\n",
      "-------------------------------------------\n",
      "training time: 0.0019969940185546875 seconds\n",
      "testing time: 0.0010008811950683594 seconds\n",
      "accuracy: 0.7676767676767676\n",
      "micro f-score: 0.7676767676767676\n",
      "macro f-score: 0.7405719494132392\n",
      "weighted f-score: 0.7752999978133851\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [1000] min_samples_leaf: [1]\n",
      "-------------------------------------------\n",
      "training time: 0.0009996891021728516 seconds\n",
      "testing time: 0.0 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [1000] min_samples_leaf: [50]\n",
      "-------------------------------------------\n",
      "training time: 0.003990650177001953 seconds\n",
      "testing time: 0.0010066032409667969 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [1000] min_samples_leaf: [100]\n",
      "-------------------------------------------\n",
      "training time: 0.0020012855529785156 seconds\n",
      "testing time: 0.0 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [1000] min_samples_leaf: [1000]\n",
      "-------------------------------------------\n",
      "training time: 0.0010006427764892578 seconds\n",
      "testing time: 0.003014087677001953 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "mean training time: 0.0028121471405029297\n",
      "mean testing time: 0.0011271685361862183\n",
      "mean accuracy_measures: 0.7348484848484849\n",
      "mean weighted_f1_measures: 0.7090408432908479\n"
     ]
    }
   ],
   "source": [
    "# result analysis helper lists\n",
    "training_times = []\n",
    "test_times = []\n",
    "accuracy_measures = []\n",
    "weithged_f1_measures = []\n",
    "\n",
    "# algo input parameter variation lists\n",
    "min_samples_splits = [2, 50, 100, 1000]\n",
    "min_samples_leafs = [1, 50, 100, 1000]\n",
    "\n",
    "for min_samples_split in min_samples_splits:\n",
    "    for min_samples_leaf in min_samples_leafs:\n",
    "        algo = DecisionTreeClassifier(criterion='gini', splitter='best', min_samples_split=min_samples_split, random_state=547998)\n",
    "\n",
    "        start_training = time.time()\n",
    "        modle = algo.fit(X=X_train, y=y_train)\n",
    "        training_times.append(time.time() - start_training)\n",
    "\n",
    "        start_testing = time.time()\n",
    "        y_pred = modle.predict(X=X_test)\n",
    "        test_times.append(time.time() - start_testing)\n",
    "\n",
    "        accuracy_measures.append(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "        weithged_f1_measures.append(f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "\n",
    "        print(\"Run-parameters min_samples_split: [\" + str(min_samples_split) + \"] min_samples_leaf: [\" + str(min_samples_leaf) + \"]\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(\"training time: \" + str(training_times[-1]) + \" seconds\")\n",
    "        print(\"testing time: \" + str(test_times[-1]) + \" seconds\")\n",
    "\n",
    "        print(\"accuracy: \" + str(accuracy_measures[-1]))\n",
    "        print(\"micro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='micro')))\n",
    "        print(\"macro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='macro')))\n",
    "        print(\"weighted f-score: \" + str(weithged_f1_measures[-1]))\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "        # crosschecking results\n",
    "        # print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "mean_training_time = statistics.mean(training_times)\n",
    "mean_testing_time = statistics.mean(test_times)\n",
    "mean_accuracy_measure = statistics.mean(accuracy_measures)\n",
    "mean_weighted_f1_measure = statistics.mean(weithged_f1_measures)\n",
    "\n",
    "print(\"mean training time: \" + str(mean_training_time))\n",
    "print(\"mean testing time: \" + str(mean_testing_time))\n",
    "print(\"mean accuracy_measures: \" + str(mean_accuracy_measure))\n",
    "print(\"mean weighted_f1_measures: \" + str(mean_weighted_f1_measure))\n",
    "\n",
    "dt_mean_training_time = mean_training_time\n",
    "dt_mean_testing_time = mean_testing_time\n",
    "dt_mean_accuracy_measure = mean_accuracy_measure\n",
    "dt_mean_weighted_f1_measure = mean_weighted_f1_measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "I chose to vary the value for perameter alpha according to this article (https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html) and the value for parameter penalty, since the penalty for a failed attempt seems to have significant impact on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run-parameters penaltiy: [l2] alpha: [0.1]\n",
      "-------------------------------------------\n",
      "training time: 0.002000093460083008 seconds\n",
      "testing time: 0.0 seconds\n",
      "accuracy: 0.29292929292929293\n",
      "micro f-score: 0.29292929292929293\n",
      "macro f-score: 0.22656250000000003\n",
      "weighted f-score: 0.13273358585858588\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [0.1]\n",
      "-------------------------------------------\n",
      "training time: 0.0044209957122802734 seconds\n",
      "testing time: 0.0013239383697509766 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l2] alpha: [0.31622776601683794]\n",
      "-------------------------------------------\n",
      "training time: 0.0019927024841308594 seconds\n",
      "testing time: 0.0009980201721191406 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [0.31622776601683794]\n",
      "-------------------------------------------\n",
      "training time: 0.0010020732879638672 seconds\n",
      "testing time: 0.0009980201721191406 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l2] alpha: [1.0]\n",
      "-------------------------------------------\n",
      "training time: 0.00299835205078125 seconds\n",
      "testing time: 0.0010099411010742188 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [1.0]\n",
      "-------------------------------------------\n",
      "training time: 0.002009153366088867 seconds\n",
      "testing time: 0.0009899139404296875 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l2] alpha: [3.1622776601683795]\n",
      "-------------------------------------------\n",
      "training time: 0.005003690719604492 seconds\n",
      "testing time: 0.0020089149475097656 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [3.1622776601683795]\n",
      "-------------------------------------------\n",
      "training time: 0.0019884109497070312 seconds\n",
      "testing time: 0.0010068416595458984 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l2] alpha: [10.0]\n",
      "-------------------------------------------\n",
      "training time: 0.0020017623901367188 seconds\n",
      "testing time: 0.0010111331939697266 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [10.0]\n",
      "-------------------------------------------\n",
      "training time: 0.002005338668823242 seconds\n",
      "testing time: 0.0009996891021728516 seconds\n",
      "accuracy: 0.7070707070707071\n",
      "micro f-score: 0.7070707070707071\n",
      "macro f-score: 0.4142011834319526\n",
      "weighted f-score: 0.5857390472775087\n",
      "-------------------------------------------\n",
      "mean training time: 0.002542257308959961\n",
      "mean testing time: 0.0010346412658691407\n",
      "mean accuracy_measures: 0.6656565656565656\n",
      "mean weighted_f1_measures: 0.5404385011356164\n"
     ]
    }
   ],
   "source": [
    "# result analysis helper lists\n",
    "training_times = []\n",
    "test_times = []\n",
    "accuracy_measures = []\n",
    "weithged_f1_measures = []\n",
    "\n",
    "# algo input parameter variation lists\n",
    "alphas = np.logspace(-1, 1, 5)\n",
    "penalties = ['l2', 'l1']\n",
    "\n",
    "for alpha in alphas:\n",
    "    for penalty in penalties:\n",
    "        algo = Perceptron(alpha=alpha, penalty=penalty, random_state=547998)\n",
    "\n",
    "        start_training = time.time()\n",
    "        modle = algo.fit(X=X_train, y=y_train.values.ravel())\n",
    "        training_times.append(time.time() - start_training)\n",
    "\n",
    "        start_testing = time.time()\n",
    "        y_pred = modle.predict(X=X_test)\n",
    "        test_times.append(time.time() - start_testing)\n",
    "\n",
    "        accuracy_measures.append(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "        weithged_f1_measures.append(f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "\n",
    "        print(\"Run-parameters penaltiy: [\" + str(penalty) + \"] alpha: [\" + str(alpha) + \"]\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(\"training time: \" + str(training_times[-1]) + \" seconds\")\n",
    "        print(\"testing time: \" + str(test_times[-1]) + \" seconds\")\n",
    "\n",
    "        print(\"accuracy: \" + str(accuracy_measures[-1]))\n",
    "        print(\"micro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='micro')))\n",
    "        print(\"macro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='macro')))\n",
    "        print(\"weighted f-score: \" + str(weithged_f1_measures[-1]))\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "        # crosschecking results\n",
    "        # print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "mean_training_time = statistics.mean(training_times)\n",
    "mean_testing_time = statistics.mean(test_times)\n",
    "mean_accuracy_measure = statistics.mean(accuracy_measures)\n",
    "mean_weighted_f1_measure = statistics.mean(weithged_f1_measures)\n",
    "\n",
    "print(\"mean training time: \" + str(mean_training_time))\n",
    "print(\"mean testing time: \" + str(mean_testing_time))\n",
    "print(\"mean accuracy_measures: \" + str(mean_accuracy_measure))\n",
    "print(\"mean weighted_f1_measures: \" + str(mean_weighted_f1_measure))\n",
    "\n",
    "p_mean_training_time = mean_training_time\n",
    "p_mean_testing_time = mean_testing_time\n",
    "p_mean_accuracy_measure = mean_accuracy_measure\n",
    "p_mean_weighted_f1_measure = mean_weighted_f1_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running tests with different algorithms, kd-tree algorithm worked the best (fastest). Other algorithms took too long to be reasonably evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run-parameters n_neighbors: [3]\n",
      "-------------------------------------------\n",
      "training time: 0.002000570297241211 seconds\n",
      "testing time: 0.008002758026123047 seconds\n",
      "accuracy: 0.5555555555555556\n",
      "micro f-score: 0.5555555555555556\n",
      "macro f-score: 0.4398148148148148\n",
      "weighted f-score: 0.5452674897119342\n",
      "-------------------------------------------\n",
      "Run-parameters n_neighbors: [5]\n",
      "-------------------------------------------\n",
      "training time: 0.0009989738464355469 seconds\n",
      "testing time: 0.002991914749145508 seconds\n",
      "accuracy: 0.6565656565656566\n",
      "micro f-score: 0.6565656565656566\n",
      "macro f-score: 0.5185926773455377\n",
      "weighted f-score: 0.6253264914592145\n",
      "-------------------------------------------\n",
      "Run-parameters n_neighbors: [10]\n",
      "-------------------------------------------\n",
      "training time: 0.003000020980834961 seconds\n",
      "testing time: 0.004000663757324219 seconds\n",
      "accuracy: 0.696969696969697\n",
      "micro f-score: 0.696969696969697\n",
      "macro f-score: 0.4907407407407407\n",
      "weighted f-score: 0.6249532360643472\n",
      "-------------------------------------------\n",
      "mean training time: 0.0019998550415039062\n",
      "mean testing time: 0.004998445510864258\n",
      "mean accuracy_measures: 0.6363636363636364\n",
      "mean weighted_f1_measures: 0.5985157390784986\n"
     ]
    }
   ],
   "source": [
    "# result analysis helper lists\n",
    "training_times = []\n",
    "test_times = []\n",
    "accuracy_measures = []\n",
    "weithged_f1_measures = []\n",
    "\n",
    "# algo input parameter variation lists\n",
    "neighbors = [3, 5, 10]\n",
    "\n",
    "for n_neighbors in neighbors:\n",
    "    algo = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree')\n",
    "\n",
    "    start_training = time.time()\n",
    "    modle = algo.fit(X=X_train, y=y_train.values.ravel())\n",
    "    training_times.append(time.time() - start_training)\n",
    "\n",
    "    start_testing = time.time()\n",
    "    y_pred = modle.predict(X=X_test)\n",
    "    test_times.append(time.time() - start_testing)\n",
    "\n",
    "    accuracy_measures.append(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "    weithged_f1_measures.append(f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    \n",
    "    print(\"Run-parameters n_neighbors: [\" + str(n_neighbors) + \"]\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"training time: \" + str(training_times[-1]) + \" seconds\")\n",
    "    print(\"testing time: \" + str(test_times[-1]) + \" seconds\")\n",
    "\n",
    "    print(\"accuracy: \" + str(accuracy_measures[-1]))\n",
    "    print(\"micro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='micro')))\n",
    "    print(\"macro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='macro')))\n",
    "    print(\"weighted f-score: \" + str(weithged_f1_measures[-1]))\n",
    "    print(\"-------------------------------------------\")\n",
    "    # crosschecking results\n",
    "    # print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "mean_training_time = statistics.mean(training_times)\n",
    "mean_testing_time = statistics.mean(test_times)\n",
    "mean_accuracy_measure = statistics.mean(accuracy_measures)\n",
    "mean_weighted_f1_measure = statistics.mean(weithged_f1_measures)\n",
    "\n",
    "print(\"mean training time: \" + str(mean_training_time))\n",
    "print(\"mean testing time: \" + str(mean_testing_time))\n",
    "print(\"mean accuracy_measures: \" + str(mean_accuracy_measure))\n",
    "print(\"mean weighted_f1_measures: \" + str(mean_weighted_f1_measure))\n",
    "\n",
    "knn_mean_training_time = mean_training_time\n",
    "knn_mean_testing_time = mean_testing_time\n",
    "knn_mean_accuracy_measure = mean_accuracy_measure\n",
    "knn_mean_weighted_f1_measure = mean_weighted_f1_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------+-----------------+----------------+\n",
      "| Coverage      |   Accuracy |       F1 |   Training time |   Testing time |\n",
      "+===============+============+==========+=================+================+\n",
      "| K-NN          |   0.636364 | 0.598516 |      0.00199986 |     0.00499845 |\n",
      "+---------------+------------+----------+-----------------+----------------+\n",
      "| Perceptron    |   0.665657 | 0.540439 |      0.00254226 |     0.00103464 |\n",
      "+---------------+------------+----------+-----------------+----------------+\n",
      "| Decision Tree |   0.734848 | 0.709041 |      0.00281215 |     0.00112717 |\n",
      "+---------------+------------+----------+-----------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "headers = [\"Coverage\", \"Accuracy\", \"F1\", \"Training time\", \"Testing time\"]\n",
    "\n",
    "table_data = [\n",
    "    [\"K-NN\", str(knn_mean_accuracy_measure), str(knn_mean_weighted_f1_measure), str(knn_mean_training_time), str(knn_mean_testing_time)],\n",
    "    [\"Perceptron\", str(p_mean_accuracy_measure), str(p_mean_weighted_f1_measure), str(p_mean_training_time), str(p_mean_testing_time)],\n",
    "    [\"Decision Tree\", str(dt_mean_accuracy_measure), str(dt_mean_weighted_f1_measure), str(dt_mean_training_time), str(dt_mean_testing_time)],\n",
    "]\n",
    "\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the small dataset, execution times are negligible. All three methods require so little time to be trained and tested that the results appear to be produced instantaneously. Decision tree produces models that provide the highest accuracy values. K-NN and Perceptron's accuracy can be considered insufficient. However, even Decision tree's Accuracy and F1 values are not as good as they have been with the large dataset."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41fa8fe2831d258fbbd985d2e88a2b9b6134a1d6f55ea2eeb00e6d0742fd7957"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
