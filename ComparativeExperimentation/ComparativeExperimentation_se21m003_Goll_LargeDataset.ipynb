{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as time\n",
    "import statistics\n",
    "from tabulate import tabulate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2959.365301</td>\n",
       "      <td>155.656807</td>\n",
       "      <td>14.103704</td>\n",
       "      <td>269.428217</td>\n",
       "      <td>46.418855</td>\n",
       "      <td>2350.146611</td>\n",
       "      <td>212.146049</td>\n",
       "      <td>223.318716</td>\n",
       "      <td>142.528263</td>\n",
       "      <td>1980.291226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090392</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.026803</td>\n",
       "      <td>0.023762</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>2.051471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>279.984734</td>\n",
       "      <td>111.913721</td>\n",
       "      <td>7.488242</td>\n",
       "      <td>212.549356</td>\n",
       "      <td>58.295232</td>\n",
       "      <td>1559.254870</td>\n",
       "      <td>26.769889</td>\n",
       "      <td>19.768697</td>\n",
       "      <td>38.274529</td>\n",
       "      <td>1324.195210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286743</td>\n",
       "      <td>0.267725</td>\n",
       "      <td>0.052584</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>0.161508</td>\n",
       "      <td>0.152307</td>\n",
       "      <td>0.121791</td>\n",
       "      <td>1.396504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1859.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-173.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2809.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2996.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1710.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3163.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>3328.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3858.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1397.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>7117.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>7173.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0              1              2              3   \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean     2959.365301     155.656807      14.103704     269.428217   \n",
       "std       279.984734     111.913721       7.488242     212.549356   \n",
       "min      1859.000000       0.000000       0.000000       0.000000   \n",
       "25%      2809.000000      58.000000       9.000000     108.000000   \n",
       "50%      2996.000000     127.000000      13.000000     218.000000   \n",
       "75%      3163.000000     260.000000      18.000000     384.000000   \n",
       "max      3858.000000     360.000000      66.000000    1397.000000   \n",
       "\n",
       "                  4              5              6              7   \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean       46.418855    2350.146611     212.146049     223.318716   \n",
       "std        58.295232    1559.254870      26.769889      19.768697   \n",
       "min      -173.000000       0.000000       0.000000       0.000000   \n",
       "25%         7.000000    1106.000000     198.000000     213.000000   \n",
       "50%        30.000000    1997.000000     218.000000     226.000000   \n",
       "75%        69.000000    3328.000000     231.000000     237.000000   \n",
       "max       601.000000    7117.000000     254.000000     254.000000   \n",
       "\n",
       "                  8              9   ...             45             46  \\\n",
       "count  581012.000000  581012.000000  ...  581012.000000  581012.000000   \n",
       "mean      142.528263    1980.291226  ...       0.090392       0.077716   \n",
       "std        38.274529    1324.195210  ...       0.286743       0.267725   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%       119.000000    1024.000000  ...       0.000000       0.000000   \n",
       "50%       143.000000    1710.000000  ...       0.000000       0.000000   \n",
       "75%       168.000000    2550.000000  ...       0.000000       0.000000   \n",
       "max       254.000000    7173.000000  ...       1.000000       1.000000   \n",
       "\n",
       "                  47             48             49             50  \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean        0.002773       0.003255       0.000205       0.000513   \n",
       "std         0.052584       0.056957       0.014310       0.022641   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  51             52             53             54  \n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000  \n",
       "mean        0.026803       0.023762       0.015060       2.051471  \n",
       "std         0.161508       0.152307       0.121791       1.396504  \n",
       "min         0.000000       0.000000       0.000000       1.000000  \n",
       "25%         0.000000       0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       0.000000       2.000000  \n",
       "75%         0.000000       0.000000       0.000000       2.000000  \n",
       "max         1.000000       1.000000       1.000000       7.000000  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'G:\\My Drive\\FH_Technikum\\MSC\\Semester_2_SS2022\\DAS\\ComparativeExperimentation\\covtype.data', header=None)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testdata as well as columns that should be predicted (y) and columns that contain data that will be used to predict (X). (= holdout method)\n",
    "\n",
    "The columns that should be predicted (target/dependent) must be excluded from the trainingsdata to not influence the created modle.\n",
    "\n",
    "The dependent (to be predicted) data is located in column 54 (Forest Cover Type Classes => values from 1 to 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (389278, 54)\n",
      "X_test: (191734, 54)\n",
      "y_train: (389278, 1)\n",
      "y_test: (191734, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.loc[:,:53], data.loc[:,54:], test_size=0.33, random_state=547998)\n",
    "print(\"X_train: \" + str(X_train.shape))\n",
    "print(\"X_test: \" + str(X_test.shape))\n",
    "print(\"y_train: \" + str(y_train.shape))\n",
    "print(\"y_test: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "I chose to vary the parameters for min_samples_splits and min_samples_leafs to see the differences between the different values since they seem to be the most promising to have an impact on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run-parameters min_samples_split: [2] min_samples_leaf: [1]\n",
      "-------------------------------------------\n",
      "training time: 5.714843034744263 seconds\n",
      "testing time: 0.08300065994262695 seconds\n",
      "accuracy: 0.9340805490940574\n",
      "micro f-score: 0.9340805490940574\n",
      "macro f-score: 0.8951065850043266\n",
      "weighted f-score: 0.9340784980380927\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [2] min_samples_leaf: [50]\n",
      "-------------------------------------------\n",
      "training time: 5.55415678024292 seconds\n",
      "testing time: 0.07399892807006836 seconds\n",
      "accuracy: 0.9340805490940574\n",
      "micro f-score: 0.9340805490940574\n",
      "macro f-score: 0.8951065850043266\n",
      "weighted f-score: 0.9340784980380927\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [2] min_samples_leaf: [100]\n",
      "-------------------------------------------\n",
      "training time: 6.591277599334717 seconds\n",
      "testing time: 0.1358959674835205 seconds\n",
      "accuracy: 0.9340805490940574\n",
      "micro f-score: 0.9340805490940574\n",
      "macro f-score: 0.8951065850043266\n",
      "weighted f-score: 0.9340784980380927\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [2] min_samples_leaf: [1000]\n",
      "-------------------------------------------\n",
      "training time: 7.507266998291016 seconds\n",
      "testing time: 0.08899760246276855 seconds\n",
      "accuracy: 0.9340805490940574\n",
      "micro f-score: 0.9340805490940574\n",
      "macro f-score: 0.8951065850043266\n",
      "weighted f-score: 0.9340784980380927\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [50] min_samples_leaf: [1]\n",
      "-------------------------------------------\n",
      "training time: 6.424204349517822 seconds\n",
      "testing time: 0.09480953216552734 seconds\n",
      "accuracy: 0.9013164071056776\n",
      "micro f-score: 0.9013164071056777\n",
      "macro f-score: 0.8436928040847285\n",
      "weighted f-score: 0.9010668682381788\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [50] min_samples_leaf: [50]\n",
      "-------------------------------------------\n",
      "training time: 6.208899736404419 seconds\n",
      "testing time: 0.0730142593383789 seconds\n",
      "accuracy: 0.9013164071056776\n",
      "micro f-score: 0.9013164071056777\n",
      "macro f-score: 0.8436928040847285\n",
      "weighted f-score: 0.9010668682381788\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [50] min_samples_leaf: [100]\n",
      "-------------------------------------------\n",
      "training time: 6.201272249221802 seconds\n",
      "testing time: 0.0839846134185791 seconds\n",
      "accuracy: 0.9013164071056776\n",
      "micro f-score: 0.9013164071056777\n",
      "macro f-score: 0.8436928040847285\n",
      "weighted f-score: 0.9010668682381788\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [50] min_samples_leaf: [1000]\n",
      "-------------------------------------------\n",
      "training time: 5.97284460067749 seconds\n",
      "testing time: 0.07499146461486816 seconds\n",
      "accuracy: 0.9013164071056776\n",
      "micro f-score: 0.9013164071056777\n",
      "macro f-score: 0.8436928040847285\n",
      "weighted f-score: 0.9010668682381788\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [100] min_samples_leaf: [1]\n",
      "-------------------------------------------\n",
      "training time: 6.555252552032471 seconds\n",
      "testing time: 0.08473038673400879 seconds\n",
      "accuracy: 0.8800995128667842\n",
      "micro f-score: 0.8800995128667842\n",
      "macro f-score: 0.8154503806276548\n",
      "weighted f-score: 0.8796150885933521\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [100] min_samples_leaf: [50]\n",
      "-------------------------------------------\n",
      "training time: 7.225893259048462 seconds\n",
      "testing time: 0.08160114288330078 seconds\n",
      "accuracy: 0.8800995128667842\n",
      "micro f-score: 0.8800995128667842\n",
      "macro f-score: 0.8154503806276548\n",
      "weighted f-score: 0.8796150885933521\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [100] min_samples_leaf: [100]\n",
      "-------------------------------------------\n",
      "training time: 6.098198175430298 seconds\n",
      "testing time: 0.07900142669677734 seconds\n",
      "accuracy: 0.8800995128667842\n",
      "micro f-score: 0.8800995128667842\n",
      "macro f-score: 0.8154503806276548\n",
      "weighted f-score: 0.8796150885933521\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [100] min_samples_leaf: [1000]\n",
      "-------------------------------------------\n",
      "training time: 6.066878795623779 seconds\n",
      "testing time: 0.07999181747436523 seconds\n",
      "accuracy: 0.8800995128667842\n",
      "micro f-score: 0.8800995128667842\n",
      "macro f-score: 0.8154503806276548\n",
      "weighted f-score: 0.8796150885933521\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [1000] min_samples_leaf: [1]\n",
      "-------------------------------------------\n",
      "training time: 5.011541128158569 seconds\n",
      "testing time: 0.06658196449279785 seconds\n",
      "accuracy: 0.7935681725724181\n",
      "micro f-score: 0.7935681725724181\n",
      "macro f-score: 0.667716842589014\n",
      "weighted f-score: 0.7901624206468575\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [1000] min_samples_leaf: [50]\n",
      "-------------------------------------------\n",
      "training time: 4.910423278808594 seconds\n",
      "testing time: 0.06399941444396973 seconds\n",
      "accuracy: 0.7935681725724181\n",
      "micro f-score: 0.7935681725724181\n",
      "macro f-score: 0.667716842589014\n",
      "weighted f-score: 0.7901624206468575\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [1000] min_samples_leaf: [100]\n",
      "-------------------------------------------\n",
      "training time: 4.988515615463257 seconds\n",
      "testing time: 0.06500005722045898 seconds\n",
      "accuracy: 0.7935681725724181\n",
      "micro f-score: 0.7935681725724181\n",
      "macro f-score: 0.667716842589014\n",
      "weighted f-score: 0.7901624206468575\n",
      "-------------------------------------------\n",
      "Run-parameters min_samples_split: [1000] min_samples_leaf: [1000]\n",
      "-------------------------------------------\n",
      "training time: 4.953925371170044 seconds\n",
      "testing time: 0.06299853324890137 seconds\n",
      "accuracy: 0.7935681725724181\n",
      "micro f-score: 0.7935681725724181\n",
      "macro f-score: 0.667716842589014\n",
      "weighted f-score: 0.7901624206468575\n",
      "-------------------------------------------\n",
      "mean training time: 5.99908709526062\n",
      "mean testing time: 0.08078736066818237\n",
      "mean accuracy_measures: 0.8772661604097343\n",
      "mean weighted_f1_measures: 0.8762307188791203\n"
     ]
    }
   ],
   "source": [
    "# result analysis helper lists\n",
    "training_times = []\n",
    "test_times = []\n",
    "accuracy_measures = []\n",
    "weithged_f1_measures = []\n",
    "\n",
    "# algo input parameter variation lists\n",
    "min_samples_splits = [2, 50, 100, 1000]\n",
    "min_samples_leafs = [1, 50, 100, 1000]\n",
    "\n",
    "for min_samples_split in min_samples_splits:\n",
    "    for min_samples_leaf in min_samples_leafs:\n",
    "        algo = DecisionTreeClassifier(criterion='gini', splitter='best', min_samples_split=min_samples_split, random_state=547998)\n",
    "\n",
    "        start_training = time.time()\n",
    "        modle = algo.fit(X=X_train, y=y_train)\n",
    "        training_times.append(time.time() - start_training)\n",
    "\n",
    "        start_testing = time.time()\n",
    "        y_pred = modle.predict(X=X_test)\n",
    "        test_times.append(time.time() - start_testing)\n",
    "\n",
    "        accuracy_measures.append(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "        weithged_f1_measures.append(f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "\n",
    "        print(\"Run-parameters min_samples_split: [\" + str(min_samples_split) + \"] min_samples_leaf: [\" + str(min_samples_leaf) + \"]\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(\"training time: \" + str(training_times[-1]) + \" seconds\")\n",
    "        print(\"testing time: \" + str(test_times[-1]) + \" seconds\")\n",
    "\n",
    "        print(\"accuracy: \" + str(accuracy_measures[-1]))\n",
    "        print(\"micro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='micro')))\n",
    "        print(\"macro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='macro')))\n",
    "        print(\"weighted f-score: \" + str(weithged_f1_measures[-1]))\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "        # crosschecking results\n",
    "        # print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "mean_training_time = statistics.mean(training_times)\n",
    "mean_testing_time = statistics.mean(test_times)\n",
    "mean_accuracy_measure = statistics.mean(accuracy_measures)\n",
    "mean_weighted_f1_measure = statistics.mean(weithged_f1_measures)\n",
    "\n",
    "print(\"mean training time: \" + str(mean_training_time))\n",
    "print(\"mean testing time: \" + str(mean_testing_time))\n",
    "print(\"mean accuracy_measures: \" + str(mean_accuracy_measure))\n",
    "print(\"mean weighted_f1_measures: \" + str(mean_weighted_f1_measure))\n",
    "\n",
    "dt_mean_training_time = mean_training_time\n",
    "dt_mean_testing_time = mean_testing_time\n",
    "dt_mean_accuracy_measure = mean_accuracy_measure\n",
    "dt_mean_weighted_f1_measure = mean_weighted_f1_measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "I chose to vary the value for perameter alpha according to this article (https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html) and the value for parameter penalty, since the penalty for a failed attempt seems to have significant impact on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run-parameters penaltiy: [l2] alpha: [0.1]\n",
      "-------------------------------------------\n",
      "training time: 15.187188148498535 seconds\n",
      "testing time: 0.07785916328430176 seconds\n",
      "accuracy: 0.3645206379671837\n",
      "micro f-score: 0.36452063796718376\n",
      "macro f-score: 0.07632625759333833\n",
      "weighted f-score: 0.19475747278099903\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [0.1]\n",
      "-------------------------------------------\n",
      "training time: 15.86253547668457 seconds\n",
      "testing time: 0.08685779571533203 seconds\n",
      "accuracy: 0.464388162767167\n",
      "micro f-score: 0.464388162767167\n",
      "macro f-score: 0.19139712133510403\n",
      "weighted f-score: 0.5018217165406026\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l2] alpha: [0.31622776601683794]\n",
      "-------------------------------------------\n",
      "training time: 11.704886674880981 seconds\n",
      "testing time: 0.07594466209411621 seconds\n",
      "accuracy: 0.3645206379671837\n",
      "micro f-score: 0.36452063796718376\n",
      "macro f-score: 0.07632625759333833\n",
      "weighted f-score: 0.19475747278099903\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [0.31622776601683794]\n",
      "-------------------------------------------\n",
      "training time: 15.21473479270935 seconds\n",
      "testing time: 0.08246922492980957 seconds\n",
      "accuracy: 0.3451865605474251\n",
      "micro f-score: 0.3451865605474251\n",
      "macro f-score: 0.16681952112509232\n",
      "weighted f-score: 0.3107386339404075\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l2] alpha: [1.0]\n",
      "-------------------------------------------\n",
      "training time: 16.155882120132446 seconds\n",
      "testing time: 0.09387850761413574 seconds\n",
      "accuracy: 0.3645206379671837\n",
      "micro f-score: 0.36452063796718376\n",
      "macro f-score: 0.07632625759333833\n",
      "weighted f-score: 0.19475747278099903\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [1.0]\n",
      "-------------------------------------------\n",
      "training time: 17.496880769729614 seconds\n",
      "testing time: 0.09188127517700195 seconds\n",
      "accuracy: 0.4037416420666131\n",
      "micro f-score: 0.4037416420666131\n",
      "macro f-score: 0.15585488895434255\n",
      "weighted f-score: 0.37890528263979495\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l2] alpha: [3.1622776601683795]\n",
      "-------------------------------------------\n",
      "training time: 16.52851438522339 seconds\n",
      "testing time: 0.08653616905212402 seconds\n",
      "accuracy: 0.3645206379671837\n",
      "micro f-score: 0.36452063796718376\n",
      "macro f-score: 0.07632625759333833\n",
      "weighted f-score: 0.19475747278099903\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [3.1622776601683795]\n",
      "-------------------------------------------\n",
      "training time: 14.556557655334473 seconds\n",
      "testing time: 0.09355592727661133 seconds\n",
      "accuracy: 0.16672577633596544\n",
      "micro f-score: 0.16672577633596544\n",
      "macro f-score: 0.08094726119918763\n",
      "weighted f-score: 0.15187473607377192\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l2] alpha: [10.0]\n",
      "-------------------------------------------\n",
      "training time: 16.195900917053223 seconds\n",
      "testing time: 0.07525801658630371 seconds\n",
      "accuracy: 0.3645206379671837\n",
      "micro f-score: 0.36452063796718376\n",
      "macro f-score: 0.07632625759333833\n",
      "weighted f-score: 0.19475747278099903\n",
      "-------------------------------------------\n",
      "Run-parameters penaltiy: [l1] alpha: [10.0]\n",
      "-------------------------------------------\n",
      "training time: 29.49402117729187 seconds\n",
      "testing time: 0.09549522399902344 seconds\n",
      "accuracy: 0.027647678554664275\n",
      "micro f-score: 0.027647678554664275\n",
      "macro f-score: 0.04053018472072895\n",
      "weighted f-score: 0.023019063015406373\n",
      "-------------------------------------------\n",
      "mean training time: 16.839710211753847\n",
      "mean testing time: 0.08597359657287598\n",
      "mean accuracy_measures: 0.3230293010107753\n",
      "mean weighted_f1_measures: 0.23401467961149786\n"
     ]
    }
   ],
   "source": [
    "# result analysis helper lists\n",
    "training_times = []\n",
    "test_times = []\n",
    "accuracy_measures = []\n",
    "weithged_f1_measures = []\n",
    "\n",
    "# algo input parameter variation lists\n",
    "alphas = np.logspace(-1, 1, 5)\n",
    "penalties = ['l2', 'l1']\n",
    "\n",
    "for alpha in alphas:\n",
    "    for penalty in penalties:\n",
    "        algo = Perceptron(alpha=alpha, penalty=penalty, random_state=547998)\n",
    "\n",
    "        start_training = time.time()\n",
    "        modle = algo.fit(X=X_train, y=y_train.values.ravel())\n",
    "        training_times.append(time.time() - start_training)\n",
    "\n",
    "        start_testing = time.time()\n",
    "        y_pred = modle.predict(X=X_test)\n",
    "        test_times.append(time.time() - start_testing)\n",
    "\n",
    "        accuracy_measures.append(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "        weithged_f1_measures.append(f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "\n",
    "        print(\"Run-parameters penaltiy: [\" + str(penalty) + \"] alpha: [\" + str(alpha) + \"]\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(\"training time: \" + str(training_times[-1]) + \" seconds\")\n",
    "        print(\"testing time: \" + str(test_times[-1]) + \" seconds\")\n",
    "\n",
    "        print(\"accuracy: \" + str(accuracy_measures[-1]))\n",
    "        print(\"micro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='micro')))\n",
    "        print(\"macro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='macro')))\n",
    "        print(\"weighted f-score: \" + str(weithged_f1_measures[-1]))\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "        # crosschecking results\n",
    "        # print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "mean_training_time = statistics.mean(training_times)\n",
    "mean_testing_time = statistics.mean(test_times)\n",
    "mean_accuracy_measure = statistics.mean(accuracy_measures)\n",
    "mean_weighted_f1_measure = statistics.mean(weithged_f1_measures)\n",
    "\n",
    "print(\"mean training time: \" + str(mean_training_time))\n",
    "print(\"mean testing time: \" + str(mean_testing_time))\n",
    "print(\"mean accuracy_measures: \" + str(mean_accuracy_measure))\n",
    "print(\"mean weighted_f1_measures: \" + str(mean_weighted_f1_measure))\n",
    "\n",
    "p_mean_training_time = mean_training_time\n",
    "p_mean_testing_time = mean_testing_time\n",
    "p_mean_accuracy_measure = mean_accuracy_measure\n",
    "p_mean_weighted_f1_measure = mean_weighted_f1_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron so far exhibits the worst performance. Accuracy is considerably lower than with the Decision Tree method. Also, execution times are higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running tests with different algorithms, kd-tree algorithm worked the best (fastest). Other algorithms took too long to be reasonably evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run-parameters n_neighbors: [3]\n",
      "-------------------------------------------\n",
      "training time: 13.0683274269104 seconds\n",
      "testing time: 22.03102207183838 seconds\n",
      "accuracy: 0.9665421886572022\n",
      "micro f-score: 0.9665421886572021\n",
      "macro f-score: 0.9378683335123738\n",
      "weighted f-score: 0.966517381946269\n",
      "-------------------------------------------\n",
      "Run-parameters n_neighbors: [5]\n",
      "-------------------------------------------\n",
      "training time: 13.078352451324463 seconds\n",
      "testing time: 25.414071559906006 seconds\n",
      "accuracy: 0.9656086035862184\n",
      "micro f-score: 0.9656086035862184\n",
      "macro f-score: 0.933485280191597\n",
      "weighted f-score: 0.9655512211595585\n",
      "-------------------------------------------\n",
      "Run-parameters n_neighbors: [10]\n",
      "-------------------------------------------\n",
      "training time: 13.829754114151001 seconds\n",
      "testing time: 34.75074744224548 seconds\n",
      "accuracy: 0.9555269279314049\n",
      "micro f-score: 0.9555269279314049\n",
      "macro f-score: 0.9157449553320155\n",
      "weighted f-score: 0.9553630441510249\n",
      "-------------------------------------------\n",
      "mean training time: 13.325477997461954\n",
      "mean testing time: 27.398613691329956\n",
      "mean accuracy_measures: 0.9625592400582752\n",
      "mean weighted_f1_measures: 0.9624772157522842\n"
     ]
    }
   ],
   "source": [
    "# result analysis helper lists\n",
    "training_times = []\n",
    "test_times = []\n",
    "accuracy_measures = []\n",
    "weithged_f1_measures = []\n",
    "\n",
    "# algo input parameter variation lists\n",
    "neighbors = [3, 5, 10]\n",
    "\n",
    "for n_neighbors in neighbors:\n",
    "    algo = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree')\n",
    "\n",
    "    start_training = time.time()\n",
    "    modle = algo.fit(X=X_train, y=y_train.values.ravel())\n",
    "    training_times.append(time.time() - start_training)\n",
    "\n",
    "    start_testing = time.time()\n",
    "    y_pred = modle.predict(X=X_test)\n",
    "    test_times.append(time.time() - start_testing)\n",
    "\n",
    "    accuracy_measures.append(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "    weithged_f1_measures.append(f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    \n",
    "    print(\"Run-parameters n_neighbors: [\" + str(n_neighbors) + \"]\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"training time: \" + str(training_times[-1]) + \" seconds\")\n",
    "    print(\"testing time: \" + str(test_times[-1]) + \" seconds\")\n",
    "\n",
    "    print(\"accuracy: \" + str(accuracy_measures[-1]))\n",
    "    print(\"micro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='micro')))\n",
    "    print(\"macro f-score: \" + str(f1_score(y_true=y_test, y_pred=y_pred, average='macro')))\n",
    "    print(\"weighted f-score: \" + str(weithged_f1_measures[-1]))\n",
    "    print(\"-------------------------------------------\")\n",
    "    # crosschecking results\n",
    "    # print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "mean_training_time = statistics.mean(training_times)\n",
    "mean_testing_time = statistics.mean(test_times)\n",
    "mean_accuracy_measure = statistics.mean(accuracy_measures)\n",
    "mean_weighted_f1_measure = statistics.mean(weithged_f1_measures)\n",
    "\n",
    "print(\"mean training time: \" + str(mean_training_time))\n",
    "print(\"mean testing time: \" + str(mean_testing_time))\n",
    "print(\"mean accuracy_measures: \" + str(mean_accuracy_measure))\n",
    "print(\"mean weighted_f1_measures: \" + str(mean_weighted_f1_measure))\n",
    "\n",
    "knn_mean_training_time = mean_training_time\n",
    "knn_mean_testing_time = mean_testing_time\n",
    "knn_mean_accuracy_measure = mean_accuracy_measure\n",
    "knn_mean_weighted_f1_measure = mean_weighted_f1_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbor produces the best results consistently. The best accuracy can be achieved with 3 nearest neighbors. Execution time is however the longest. But with kd-tree algorithm, execution times are manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------+-----------------+----------------+\n",
      "| Coverage      |   Accuracy |       F1 |   Training time |   Testing time |\n",
      "+===============+============+==========+=================+================+\n",
      "| K-NN          |   0.962559 | 0.962477 |        13.3255  |     27.3986    |\n",
      "+---------------+------------+----------+-----------------+----------------+\n",
      "| Perceptron    |   0.323029 | 0.234015 |        16.8397  |      0.0859736 |\n",
      "+---------------+------------+----------+-----------------+----------------+\n",
      "| Decision Tree |   0.877266 | 0.876231 |         5.99909 |      0.0807874 |\n",
      "+---------------+------------+----------+-----------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "headers = [\"Coverage\", \"Accuracy\", \"F1\", \"Training time\", \"Testing time\"]\n",
    "\n",
    "table_data = [\n",
    "    [\"K-NN\", str(knn_mean_accuracy_measure), str(knn_mean_weighted_f1_measure), str(knn_mean_training_time), str(knn_mean_testing_time)],\n",
    "    [\"Perceptron\", str(p_mean_accuracy_measure), str(p_mean_weighted_f1_measure), str(p_mean_training_time), str(p_mean_testing_time)],\n",
    "    [\"Decision Tree\", str(dt_mean_accuracy_measure), str(dt_mean_weighted_f1_measure), str(dt_mean_training_time), str(dt_mean_testing_time)],\n",
    "]\n",
    "\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that K-NN is the slowest and Perceptron produces the model with the least accurate predictions. K-NN has both, long testing and training times. However, K-NN also creates the model which classifies best."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41fa8fe2831d258fbbd985d2e88a2b9b6134a1d6f55ea2eeb00e6d0742fd7957"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
